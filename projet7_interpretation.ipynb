{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Chargement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import sklearn\n",
    "import joblib\n",
    "\n",
    "# Get scikit-learn version\n",
    "scikit_version = sklearn.__version__\n",
    "\n",
    "# Load the model\n",
    "pipe = joblib.load(\"models/model_{version}.pkl\".format(version=scikit_version))\n",
    "\n",
    "# Save the model as pickle file for the web app\n",
    "joblib.dump(pipe, \"web/models/model_{version}.pkl\".format(version=scikit_version))\n",
    "\n",
    "# display the model\n",
    "pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Préparation des données transformées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false",
    "_uuid": "e8384e3033ee466ab3c426d8330822e974ad3c1b"
   },
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "\n",
    "data_train_featured = pd.read_csv('data/cleaned/data_train_featured.csv', index_col='SK_ID_CURR')\n",
    "print('Featured training data set shape: ', data_train_featured.shape)\n",
    "data_train_featured.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Split features and targets\n",
    "target_train = data_train_featured['TARGET']\n",
    "data_train_featured = data_train_featured.drop(columns='TARGET')\n",
    "feature_names = data_train_featured.columns\n",
    "index = data_train_featured.index\n",
    "\n",
    "# Imputations\n",
    "imp_mean = SimpleImputer(strategy='mean')\n",
    "imp_mean.fit(data_train_featured)\n",
    "data_train_featured = pd.DataFrame(\n",
    "    imp_mean.transform(data_train_featured),\n",
    "    columns=feature_names,\n",
    "    index=index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get the predictions of the model (for positive class: default)\n",
    "y_train_pred = pipe.predict_proba(data_train_featured)[:,1]\n",
    "y_train_pred = pd.Series(\n",
    "    y_train_pred,\n",
    "    index=data_train_featured.index,\n",
    ")\n",
    "\n",
    "# Save the predictions\n",
    "y_train_pred.to_csv(\n",
    "    'data/cleaned/target_train_predictions.csv',\n",
    "    header='TARGET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Échantillonage (pour l'application web)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Sampling\n",
    "data_processed = data_train_featured.sample(n=5000, random_state=42)\n",
    "\n",
    "# Save the sample for web app\n",
    "data_processed.to_csv('web/data/data_processed.csv')\n",
    "\n",
    "# Display\n",
    "data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Préparation des données initiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train_original = pd.read_csv('data/input/application_train.csv', index_col='SK_ID_CURR')\n",
    "print('Original training set (application_set) shape: ', data_train_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "mask = data_train_original.index.isin(data_train_featured.index)\n",
    "\n",
    "# Keep only points that are in the engineered set\n",
    "data_train_original = data_train_original[mask]\n",
    "\n",
    "# Drop target column\n",
    "data_train_original = data_train_original.drop(columns='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Sampling\n",
    "data_original = data_train_original.sample(n=5000, random_state=42)\n",
    "\n",
    "# Save the sample for web app\n",
    "data_original.to_csv('web/data/data_original.csv')\n",
    "\n",
    "# Display\n",
    "data_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation of missing values for numerical features\n",
    "numerical_features = list(data_train_original.select_dtypes(include='number').columns)\n",
    "data_train_original[numerical_features] = SimpleImputer(strategy='mean').fit_transform(data_train_original[numerical_features])\n",
    "\n",
    "# Imputation of missing values for categorical features\n",
    "categorical_features = list(data_train_original.select_dtypes(include='object').columns)\n",
    "data_train_original[categorical_features] = SimpleImputer(strategy='constant', fill_value='missing').fit_transform(data_train_original[categorical_features])\n",
    "\n",
    "# Display results\n",
    "print(\"Remaining missing values:\", data_train_original.isna().any().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# label encoding of each categorical feature\n",
    "categorical_names = {} # dictionnary of modalities for each cat. feature\n",
    "label_encoders = {} # dictionnary of encoders for each cat. feature\n",
    "for feature in categorical_features:\n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    data_train_original.loc[:, feature] = le.fit_transform(data_train_original.loc[:, feature])\n",
    "    categorical_names[feature] = le.classes_\n",
    "    label_encoders[feature] = le\n",
    "\n",
    "# Save the processed original data\n",
    "data_train_original.to_csv('data/cleaned/data_train_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Sampling\n",
    "data_original_le = data_train_original.sample(n=5000, random_state=42)\n",
    "\n",
    "# Save the sample for web app\n",
    "data_original_le.to_csv('web/data/data_original_le.csv')\n",
    "\n",
    "# Display\n",
    "data_original_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Modèle de substitution (*Surrogate model*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Interprétation globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Instanciate a surrogate model without depth limit to overfit\n",
    "sur_dt = DecisionTreeRegressor()\n",
    "\n",
    "# Over-fitting the surrogate model on original features\n",
    "sur_dt.fit(data_train_original, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Get scikit-learn version\n",
    "scikit_version = sklearn.__version__\n",
    "\n",
    "# Save the model as pickle file\n",
    "joblib.dump(sur_dt, \"web/models/surrogate_model_{version}.pkl\".format(version=scikit_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(\"Main features per importance:\")\n",
    "\n",
    "sum_val = 0\n",
    "for col, val in sorted(zip(data_train_original.columns, sur_dt.feature_importances_,), key=lambda x: x[1], reverse=True,)[:10]:\n",
    "    print(f\"{col:28}{val:10.3f}\")\n",
    "    sum_val += val\n",
    "    \n",
    "print(\"Percentage of the model explained by the 10 first features:\", sum_val*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Interprétation locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "SK_ID_CURR = 100002\n",
    "application_data = data_train_original.loc[SK_ID_CURR:SK_ID_CURR]\n",
    "\n",
    "print(\"Application ID:\", SK_ID_CURR)\n",
    "print(\"Predicted value:\", y_train_pred.loc[SK_ID_CURR])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from treeinterpreter import treeinterpreter as ti\n",
    "\n",
    "# Computation of the prediction, bias and contribs from surrogate model\n",
    "prediction, bias, contribs = ti.predict(sur_dt, application_data)\n",
    "\n",
    "print(\"Prediction:\", prediction)\n",
    "\n",
    "print(\"Bias (trainset mean):\", bias)\n",
    "\n",
    "print(\"Main features contributions:\")\n",
    "for contrib, feature in sorted(zip(contribs[0], data_train_original.columns), key=lambda x: abs(x[0]), reverse=True,):\n",
    "    if contrib != 0:\n",
    "        print(\"   {:32}{}\".format(feature, contrib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Creating the pd.Series of features_contribs\n",
    "features_contribs = pd.Series(contribs[0], index=data_original_le.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Converting the pd.Series to JSON\n",
    "features_contribs_json = json.loads(features_contribs.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from flask import jsonify\n",
    "\n",
    "# Returning the processed data\n",
    "jsonify({\n",
    "        'status': 'ok',\n",
    "        'prediction': prediction,\n",
    "        'bias': bias[0],\n",
    "        'contribs': features_contribs_json,\n",
    "     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "prediction[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# sns.set()\n",
    "\n",
    "# initialization\n",
    "sum_val = 0\n",
    "labels = []\n",
    "frequencies = []\n",
    "\n",
    "# get the labels and frequencies of 10 most important features\n",
    "for col, val in sorted(zip(data_train_original.columns, sur_dt.feature_importances_,), key=lambda x: x[1], reverse=True,)[:9]:\n",
    "    labels.append(col)\n",
    "    frequencies.append(val)\n",
    "    sum_val += val\n",
    "\n",
    "# complete the data with other features\n",
    "labels.append(\"OTHER FEATURES…\")\n",
    "frequencies.append(1 - sum_val)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"equal\")\n",
    "ax.pie(frequencies,\n",
    "        # autopct=\"%1.1f pourcents\",\n",
    "      )\n",
    "plt.title(\"Features importance\")\n",
    "plt.legend(\n",
    "    labels,\n",
    "    loc='center left',\n",
    "    bbox_to_anchor=(1, 0.5),\n",
    ")\n",
    "plt.show()\n",
    "fig.savefig('plots/FI.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "type(sur_dt.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_train_original = data_train_original.drop(columns='TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "features_names = data_train_original.columns\n",
    "features_importance = sur_dt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "features_importance = pd.Series(sur_dt.feature_importances_, index=data_train_original.columns).sort_values(ascending=False)\n",
    "features_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Description des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Loading the file with descriptions\n",
    "features_descriptions = pd.read_csv('data/HomeCredit_columns_description.csv', encoding='iso-8859-1')\n",
    "features_descriptions.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# filtering features from 'application_train' table\n",
    "mask = features_descriptions['Table'] == 'application_{train|test}.csv'\n",
    "features_descriptions = features_descriptions[mask]\n",
    "\n",
    "# setting the name of the feature as index\n",
    "features_descriptions = features_descriptions.set_index('Row')\n",
    "\n",
    "# keeping only description\n",
    "features_descriptions = features_descriptions['Description']\n",
    "\n",
    "# display result\n",
    "features_descriptions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Checking the result\n",
    "for column in data_train_original.columns[:5]:\n",
    "    print(column, features_descriptions[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Save the data for web app\n",
    "features_descriptions.to_csv('web/data/features_descriptions.csv', header='description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "features_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Données aggrégées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_train_original = pd.read_csv('data/input/application_train.csv', index_col='SK_ID_CURR')\n",
    "print('Original training set (application_set) shape: ', data_train_original.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_train_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Aggregate the data from loan applications\n",
    "data_agg_num = data_train_original.mean(numeric_only=True)\n",
    "data_agg_cat = data_train_original.select_dtypes(exclude='number').mode().iloc[0]\n",
    "data_agg = pd.concat([data_agg_num, data_agg_cat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Drop the target\n",
    "data_agg = data_agg.drop('TARGET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Save the data for web app\n",
    "data_agg.to_csv('web/data/data_agg.csv', header='mean or mode', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# aggregated data of the train set for comparison to current applicant\n",
    "data_agg = pd.read_csv(\"web/data/data_agg.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
